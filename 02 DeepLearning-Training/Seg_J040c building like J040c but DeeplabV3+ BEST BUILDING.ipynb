{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksMN1dbJbiTZ"
   },
   "source": [
    "# Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "654QopxlNw0u"
   },
   "outputs": [],
   "source": [
    "VERSION = \"J040c\"\n",
    "\n",
    "# do we run on Jarvis cloud platform?\n",
    "#JARVIS = True\n",
    "JARVIS = False\n",
    "\n",
    "TESTRUN = False#True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GqP0-3GbiTa",
    "outputId": "92b86015-8e2a-476a-fb66-e576a352f497"
   },
   "outputs": [],
   "source": [
    "#Run this once per session\n",
    "!pip install fastai -q --upgrade\n",
    "!pip install git+https://github.com/WaterKnight1998/SemTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unyF_YP0biTc"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43-zWMEYbiTc"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "import gc # garbage collector\n",
    "\n",
    "# SemTorch\n",
    "from semtorch import get_segmentation_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix randomness\n",
    "my_seed = 42\n",
    "np.random.seed(my_seed);random.seed(my_seed);set_seed(my_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yv9X04iNw0w"
   },
   "source": [
    "# Choose class used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImyXnyUt47gZ"
   },
   "outputs": [],
   "source": [
    "# what class are we looking for?\n",
    "#myclass = \"aguada\"\n",
    "myclass = \"building\"\n",
    "#myclass = \"platform\"\n",
    "\n",
    "mymask = \"mask_\"+myclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CH9mEmgybiTc"
   },
   "source": [
    "# Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path(\"../Data/\")\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zH5Yelner01"
   },
   "outputs": [],
   "source": [
    "path = root_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7XTNYHpbiTe"
   },
   "source": [
    "Some paths we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xawJJEhfbiTe"
   },
   "outputs": [],
   "source": [
    "path_im = path/'lidar_train'\n",
    "# these are the original masks but 'normalized' for fastai (i.e. only 0 and 1 allowed)\n",
    "path_lbl = path/'train_masks_normalized'\n",
    "\n",
    "# this contains the pixel counts\n",
    "maya_csv = path/'maya_analysis.csv'\n",
    "\n",
    "if myclass == 'aguada': ### ????????????????????????\n",
    "    # old synthetic images\n",
    "    synth_path_im = path/'03_generated/images'\n",
    "    synth_path_lbl = path/'03_generated/normalized_masks'\n",
    "else:\n",
    "    # new synthetic images (no padding) for BUILDING\n",
    "    synth_path_im = path/'03_generated_21-05-20/images'\n",
    "    synth_path_lbl = path/'03_generated_21-05-20/normalized_masks'\n",
    "    \n",
    "    # newest (rectangular cut) synthetic images\n",
    "    #synth_path_im = path/'03_generated_21-06-07/images'\n",
    "    #synth_path_lbl = path/'03_generated_21-06-07/normalized_masks'\n",
    "\n",
    "    # newest synthetic images (pad 8 px)\n",
    "    #synth_path_im = path/'03_generated_21-06-14_padded/images'\n",
    "    #synth_path_lbl = path/'03_generated_21-06-14_padded/normalized_masks'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note:\n",
    "\n",
    "The data collection below is much more complicated that it should be. This is because we tried some data filtering first, but that did not work out. However there was no time to remove this code. We will remove it before the ECML conference, however.\n",
    "\n",
    "In essence you will notice that we create a pandas dataframe containing all images and all masks (i.e  do *not* use the option to remove any) and add the synthetic images+masks.\n",
    "\n",
    "(It is based on a csv file named maya_analysis.csv which contains a pixel count for each mask. But we do not use that information, as it turned out to be not helping the training if we use Focal Loss as loss function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LKuzRnlbiTf"
   },
   "source": [
    "First we collect our filenames. Only these with nonzero data in their mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "EWW19Fkl1M0W",
    "outputId": "60f15e00-5dd8-415a-bde6-af0bb08698e8"
   },
   "outputs": [],
   "source": [
    "mymaskDF = pd.read_csv(maya_csv)\n",
    "mymaskDF['fpath'] = str(path_im) + '/' +mymaskDF['name']\n",
    "print (len(mymaskDF))\n",
    "mymaskDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "ckdCYQQd1M2-",
    "outputId": "093b9b20-fe8b-4272-e8f5-04fab2997b09"
   },
   "outputs": [],
   "source": [
    "# get only these with nonzero pixCount of our class\n",
    "mydf = mymaskDF[(mymaskDF['class'] == myclass) & (mymaskDF['pixCount'] > 0)]\n",
    "print (len(mydf))\n",
    "mydf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we add the synthesized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "synthDF = pd.DataFrame(glob.glob(str(synth_path_im/myclass)+'/*.*'), columns = [\"name\"])\n",
    "synthDF['class'] = myclass\n",
    "synthDF['pixCount'] = -1 # dummy marker for 'generated'\n",
    "synthDF['fpath'] = synthDF['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(synthDF))\n",
    "synthDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "ckdCYQQd1M2-",
    "outputId": "093b9b20-fe8b-4272-e8f5-04fab2997b09"
   },
   "outputs": [],
   "source": [
    "# get some more from the zero pixCount images\n",
    "keep = 1.  # we keep 100% so the filtering is not used as stated above :-)\n",
    "otherdf = mymaskDF[(mymaskDF['class'] == myclass) & (mymaskDF['pixCount'] == 0)].sample(frac=keep)\n",
    "print (len(otherdf))\n",
    "otherdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and oversample \"good\" cases (only needed for Aguada which we oversample 6-fold)\n",
    "if (myclass == \"aguada\"):\n",
    "    mydf = mydf.sample(n=6*64, replace=True)\n",
    "print (len(mydf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MERGE HERE\n",
    "mydf = mydf.append(otherdf, ignore_index=True, sort=False)\n",
    "print (len(mydf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now add the synthesized images, too\n",
    "mydf = mydf.append(synthDF, ignore_index=True, sort=False)\n",
    "print (len(mydf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsjrlWPIbiTf",
    "outputId": "db3cda1d-e591-48f3-c078-2033cc51a0a4"
   },
   "outputs": [],
   "source": [
    "#fnames = [path_im/f for f in mydf['name']]\n",
    "fnames = [Path(f) for f in mydf['fpath']]\n",
    "len(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psMIjyZMbiTf"
   },
   "source": [
    "And now let's look at the data to see if everything is ok:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "jQusOUXdbiTf",
    "outputId": "0ea6038b-bb1a-451f-9a37-43275af0d39d"
   },
   "outputs": [],
   "source": [
    "# tile 44 contains all three classes\n",
    "img_fn = Path(root_dir/'lidar_train/tile_44_lidar.tif')\n",
    "img = PILImage.create(img_fn)\n",
    "img.show(figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xONifkjobiTg"
   },
   "source": [
    "**Now** let's grab our y's. They live in the `train_masks` folder and follow this naming pattern:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT7-Nqk-iEM9"
   },
   "source": [
    "tile_(n)\\_lidar.tif\n",
    "\n",
    "tile_(n)\\_mask_aguada.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAwQl3uNnzZv"
   },
   "outputs": [],
   "source": [
    "def get_msk(fn):\n",
    "    # modify for synthetic images\n",
    "    if str(fn)[-5] == \"d\": # as in \"generateD\", this is clumsy but we are short of time...!\n",
    "        return synth_path_lbl/fn.name.replace(\"lidar\", mymask)\n",
    "    else :\n",
    "        return path_lbl/fn.name.replace(\"lidar\", mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mask path for \"normal\" images\n",
    "print(fnames[0])\n",
    "get_msk(fnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test mask path for synthetic images\n",
    "print(synthDF['fpath'][0])\n",
    "get_msk(Path(synthDF['fpath'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5whYfdwNbiTg"
   },
   "source": [
    "Our masks are of type `PILMask` and we will make our gradient percentage (alpha) equal to 1 as we are not overlaying this on anything yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "eQtQv65-biTg",
    "outputId": "06aace81-37b1-4f6f-f894-818c02a8d879"
   },
   "outputs": [],
   "source": [
    "msk = PILMask.create(get_msk(img_fn))\n",
    "msk.show(figsize=(5,5), alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2u0BHpHNw02"
   },
   "source": [
    "We normalized the masks in advance for easier processing with fast.ai: They now contain only 0 (background) and 1 (class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1HbvqRtbiTh",
    "outputId": "86d274bc-54fa-4eb4-f4b2-413e588a8cab"
   },
   "outputs": [],
   "source": [
    "np.unique(tensor(msk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzTFtui4biTh"
   },
   "source": [
    "Here we name them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14kXne1cqWpL",
    "outputId": "cf611080-d105-43d9-82e5-9cb4bcdddf17"
   },
   "outputs": [],
   "source": [
    "codes = np.array(['background', mymask])\n",
    "codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am5DDIRcbiTi"
   },
   "source": [
    "### Progressive resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1sKT3NGbiTi"
   },
   "source": [
    "This first round we will train at half the image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkEPmMXubiTi",
    "outputId": "363f209f-7d5f-4aac-ede8-2b676db400f6"
   },
   "outputs": [],
   "source": [
    "sz = msk.shape; sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lb5C4eo1biTj",
    "outputId": "80e08fdf-a115-4c3f-8035-ce0f787b19bd"
   },
   "outputs": [],
   "source": [
    "half = tuple(int(x/2) for x in sz); half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38rXXQiV47gg"
   },
   "outputs": [],
   "source": [
    "# batch size\n",
    "bs = 8\n",
    "\n",
    "if myclass == \"platform\": bs = 8\n",
    "if myclass == 'building': bs = 8\n",
    "\n",
    "if JARVIS:\n",
    "    if myclass == 'aguada': bs = 80\n",
    "    elif myclass == 'building': bs = 120\n",
    "    else :                      bs = 100 # platform\n",
    "\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_transforms(imgsize):\n",
    "    item_tfms = [Resize(imgsize)]\n",
    "    batch_tfms = [Dihedral(),Brightness(0.1,p=0.25), Zoom(max_zoom=1.1,p=0.25),\n",
    "                  Normalize.from_stats(*imagenet_stats)\n",
    "                 ]\n",
    "    return item_tfms, batch_tfms\n",
    "\n",
    "if myclass == 'platform': # found to be working slightly better\n",
    "    def my_transforms(imgsize):\n",
    "        item_tfms = None \n",
    "        batch_tfms = [Resize(imgsize), Dihedral(),Brightness(0.1,p=0.25), Zoom(max_zoom=1.1,p=0.25),\n",
    "                      Normalize.from_stats(*imagenet_stats)\n",
    "                     ]\n",
    "        return item_tfms, batch_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataframe, just in case!\n",
    "mydf = mydf.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask retrieval function for dataframe rows, same as above but works with the dataframe\n",
    "def get_msk(row):\n",
    "    fn = Path(row[\"fpath\"])\n",
    "    # modify for synthetic images\n",
    "    if str(fn)[-5] == \"d\": # as in \"generateD\", this is so ugly!!\n",
    "        return synth_path_lbl/fn.name.replace(\"lidar\", mymask)\n",
    "    else :\n",
    "        return path_lbl/fn.name.replace(\"lidar\", mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for retrieving a dataloaders object for a 'fold'\n",
    "def get_data(mydf, fold, n_splits, codes, bs, item_tfms, batch_tfms):\n",
    "    length = int(len(mydf)/n_splits)\n",
    "    start = fold*length\n",
    "    \n",
    "    mydf['is_valid'] = False\n",
    "    mydf.loc[start:start+length,'is_valid'] = True\n",
    "    # the datablock   \n",
    "    dblock = DataBlock(blocks=(ImageBlock, MaskBlock(codes=codes)),\n",
    "                   #splitter=RandomSplitter(valid_pct=0.2),\n",
    "                   splitter=ColSplitter(), #!!! is_valid is in valid_ds\n",
    "                   get_x=ColReader('fpath'),\n",
    "                   get_y=get_msk, item_tfms=item_tfms, batch_tfms=batch_tfms)\n",
    "    # the dataloaders\n",
    "    dls = dblock.dataloaders(mydf, path='', bs=bs)\n",
    "    return dls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: HRnet or DeeplabV3+ with ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL51P2BMbiTm"
   },
   "outputs": [],
   "source": [
    "# optimizer: we use Ranger instead of Adam\n",
    "opt = ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEZYPzZzJ-eN"
   },
   "outputs": [],
   "source": [
    "# we were asked for IoU as metric, which is the same as Jaccard. Besides we track Dice, but do not use it.\n",
    "metrics = [Dice(), JaccardCoeff()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xwq8i7D5Nw06"
   },
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYcyRweJNw06"
   },
   "source": [
    "The Maya dataset is heavily imbalanced with only some  0.4 % of all pixels containing mask data (\"1\"). We use Focal Loss to overcome the class imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myloss_func=FocalLossFlat(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentron_splitter(model):\n",
    "    return [params(model.backbone), params(model.head)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture and backbone\n",
    "arch=\"deeplabv3+\";backbone=\"resnet101\"\n",
    "\n",
    "arch, backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an implementation of Cutmix data augmentation (https://arxiv.org/abs/1905.04899)\n",
    "# Taken from here (and modified): https://forums.fast.ai/t/implementing-cutmix-in-fastaiv2/67350/16\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "class CutMix(Callback):\n",
    "    run_after,run_valid = [Normalize],False\n",
    "    def __init__(self, alpha=1.): self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "    def begin_fit(self):\n",
    "        self.stack_y = getattr(self.learn.loss_func, 'y_int', False)\n",
    "        if self.stack_y: self.old_lf,self.learn.loss_func = self.learn.loss_func,self.lf\n",
    "\n",
    "    def after_fit(self):\n",
    "        if self.stack_y: self.learn.loss_func = self.old_lf\n",
    "\n",
    "    def begin_batch(self):\n",
    "        W, H = self.xb[0].size(3), self.xb[0].size(2)\n",
    "        \n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.x.device)\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        self.lam = lam.max(1)[0]\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.x.device)\n",
    "        xb1,self.yb1 = tuple(L(self.xb).itemgot(shuffle)),tuple(L(self.yb).itemgot(shuffle))\n",
    "        nx_dims = len(self.x.size())\n",
    "\n",
    "        rx = (self.distrib.sample((64,))*W).type(torch.long).to(self.x.device)\n",
    "        ry = (self.distrib.sample((64,))*H).type(torch.long).to(self.x.device)\n",
    "        rw = (torch.sqrt(1-self.lam)*W).to(self.x.device)\n",
    "        rh = (torch.sqrt(1-self.lam)*H).to(self.x.device)\n",
    "\n",
    "        x1 = torch.round(torch.clamp(rx-rw//2, min=0, max=W)).to(self.x.device).type(torch.long)\n",
    "        x2 = torch.round(torch.clamp(rx+rw//2, min=0, max=W)).to(self.x.device).type(torch.long)\n",
    "        y1 = torch.round(torch.clamp(ry-rh//2, min=0, max=H)).to(self.x.device).type(torch.long)\n",
    "        y2 = torch.round(torch.clamp(ry+rh//2, min=0, max=H)).to(self.x.device).type(torch.long)\n",
    "        \n",
    "        for i in range(len(x1)):\n",
    "            self.learn.xb[0][i, :, x1[i]:x2[i], y1[i]:y2[i]] = xb1[0][i, :, x1[i]:x2[i], y1[i]:y2[i]]\n",
    "        \n",
    "        self.lam = (1 - ((x2-x1)*(y2-y1))/(W*H)).type(torch.float)\n",
    "        \n",
    "        if not self.stack_y:\n",
    "            ny_dims = len(self.y.size())\n",
    "            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))\n",
    "\n",
    "    def lf(self, pred, *yb):\n",
    "        if not self.training: return self.old_lf(pred, *yb)\n",
    "        with NoneReduce(self.old_lf) as lf:\n",
    "            loss = torch.lerp(lf(pred,*self.yb1), lf(pred,*yb), self.lam)\n",
    "        return reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these callbacks are used during training. The first three monitor training progress, Cutmix is for augmentation.\n",
    "callbacks = [SaveModelCallback(monitor='jaccard_coeff'), \n",
    "             EarlyStoppingCallback(monitor='jaccard_coeff', patience=8),\n",
    "             ReduceLROnPlateau(monitor='jaccard_coeff'),\n",
    "             CutMix()\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the flat cosine annealing training loop along with fastai's default fine_tune(), so we create\n",
    "# our own fine_tune_flat(), modelled after Zachary Mueller's notebook from here: \n",
    "# https://www.kaggle.com/muellerzr/cassava-fastai-starter\n",
    "\n",
    "@patch\n",
    "def fine_tune_flat(self:Learner, epochs, base_lr=4e-3, freeze_epochs=1, lr_mult=100, pct_start=0.75, \n",
    "                   first_callbacks = [], second_callbacks = [], tofp32=False,**kwargs):\n",
    "    \"Fine-tune applied to `fit_flat_cos`\"\n",
    "    self.freeze()\n",
    "    try:\n",
    "        self.fit_flat_cos(freeze_epochs, slice(base_lr), pct_start=0.99, cbs=first_callbacks, **kwargs)\n",
    "    except: pass\n",
    "    gc.collect();torch.cuda.empty_cache()\n",
    "    base_lr /= 2\n",
    "    self.unfreeze()\n",
    "    if tofp32: self.to_fp32() # set to 32 bit\n",
    "    try:\n",
    "        self.fit_flat_cos(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, cbs=second_callbacks)\n",
    "    except: pass\n",
    "    gc.collect();torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have two training cycles, one with half size images, one with full size images:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf2Gr-INw08"
   },
   "source": [
    "## half size training cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define augmentation transforms\n",
    "item_tfms, batch_tfms = my_transforms(half)\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    print (\"----\", fold)\n",
    "    dls = get_data(mydf, fold, n_folds, codes, bs, item_tfms, batch_tfms) # get the dataloaders\n",
    "    # here we create the segmentation learner with SemTorch. The models are all pre-trained on ImageNet\n",
    "    learn = get_segmentation_learner(dls=dls, number_classes=2, segmentation_type=\"Semantic Segmentation\",\n",
    "                                 architecture_name=arch, backbone_name=backbone,\n",
    "                                 metrics=metrics,\n",
    "                                 splitter=segmentron_splitter,\n",
    "                                 opt_func=opt,\n",
    "                                 loss_func=myloss_func).to_fp16() # we use fp16 training\n",
    "    \n",
    "    #set hyperparameters. This should be moved out of the loop :-)\n",
    "    if myclass == 'aguada':\n",
    "        lr = 1e-2\n",
    "        freeze_epochs = 8#19\n",
    "        epochs = 16# 6\n",
    "    elif myclass == 'building':\n",
    "        lr = 1e-2\n",
    "        freeze_epochs = 19\n",
    "        epochs = 9\n",
    "    elif myclass == 'platform':\n",
    "        lr =1.2e-2\n",
    "        freeze_epochs = 8\n",
    "        epochs = 16\n",
    "    \n",
    "    # here is the training cycle: we train for 'freeze_epochs' with all layers (except the last) frozen,\n",
    "    # then we train for 'epochs' with all layers unfrozen \n",
    "    learn.fine_tune_flat(epochs, lr, freeze_epochs, first_callbacks=callbacks, second_callbacks=callbacks)\n",
    "    learn.recorder.plot_loss()\n",
    "    learn.export('models/stage-1 ' + myclass + str(fold))\n",
    "    print (\"--- fold complete #\", fold)\n",
    "    print (learn.validate())\n",
    "    print (\"-----------------\")\n",
    "    del dls, learn # free memory\n",
    " \n",
    "    # early stopping\n",
    "    if TESTRUN: break\n",
    "        \n",
    "# beat 69.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from jarviscloud import jarviscloud\n",
    "#jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qf2Gr-INw08"
   },
   "source": [
    "## Full size training cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sz is 480x480 now, so we need new transforms\n",
    "item_tfms, batch_tfms = my_transforms(sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "if myclass == \"building\":\n",
    "    lr = 0.02\n",
    "    freeze_epochs = 6\n",
    "    epochs = 9\n",
    "elif myclass == \"aguada\":\n",
    "    lr = 1e-2\n",
    "    freeze_epochs = 8\n",
    "    epochs = 16\n",
    "elif myclass == 'platform':\n",
    "    lr = 1e-2\n",
    "    freeze_epochs = 6\n",
    "    epochs = 7\n",
    "\n",
    "lr, freeze_epochs, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the batch size\n",
    "if arch==\"deeplabv3+\":\n",
    "    bs = 3 # 2 for Unet\n",
    "else:\n",
    "    bs = 6\n",
    "\n",
    "if JARVIS:\n",
    "    if arch==\"deeplabv3+\":\n",
    "        bs = 12 # 2 for Unet\n",
    "    else:\n",
    "        if myclass == 'aguada': bs = 16\n",
    "        elif myclass == 'building': bs = 16\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we iterate over each fold\n",
    "for fold in range(n_folds):\n",
    "    dls = get_data(mydf, fold, n_folds, codes, bs, item_tfms, batch_tfms) # get dataloaders\n",
    "    learn = load_learner('models/stage-1 ' + myclass + str(fold)) # re-load the learner\n",
    "    learn.dls = dls # and insert the new dataloaders\n",
    "    learn.to_fp16() # switch into fp16 training (half precision)\n",
    "    learn.loss_func = myloss_func # set loss function\n",
    "    # note that we now fine tune again but 2nd cycle with full precision (tofp32)\n",
    "    learn.fine_tune_flat(epochs, lr, freeze_epochs, first_callbacks=callbacks, second_callbacks=callbacks, tofp32=True)\n",
    "    learn.recorder.plot_loss()\n",
    "    MYMODEL = \"models/maya_\"+VERSION+\"_\"+myclass+str(fold)+\".pkl\"\n",
    "    print (\"--- fold complete #\", fold)\n",
    "    print (learn.validate())\n",
    "    print (\"-----------------\")\n",
    "    learn.export(MYMODEL)\n",
    "    del dls, learn\n",
    "    if TESTRUN: break\n",
    "        \n",
    "# beat 73.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from jarviscloud import jarviscloud\n",
    "#jarviscloud.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "IgfhU0zObiTq",
    "6z8pe1eqbiTv",
    "QeePnewEbiTy"
   ],
   "name": "Segmentation_010.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
